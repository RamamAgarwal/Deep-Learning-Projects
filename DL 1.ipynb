{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On Math for Deep Learning\n",
    "\n",
    "This notebook will guide you through the essential mathematical concepts that form the backbone of deep learning. You will write Python code from scratch to see these ideas in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Differentiation:\n",
    "\n",
    "Differentiation tells us the rate of change. In deep learning, we use it to find out how to adjust our model's weights to reduce error. This process is called **gradient descent**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1: Numerical Derivatives\n",
    "\n",
    "Let's work with the function $f(x) = x^3 - 2x^2 + 5$.\n",
    "\n",
    "Your task is to:\n",
    "1. Write a Python function for $f(x)$.\n",
    "2. Write a Python function for the derivative $f'(x)$.\n",
    "3. Calculate the value of the derivative at $x=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the function in Python for numerical computation\n",
    "def f(x):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "# 2. Define the derivative function\n",
    "def df(x):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "# 3. Calculate the derivative at x=2\n",
    "x_val = 2\n",
    "derivative_at_2 = df(x_val)\n",
    "\n",
    "print(f\"The value of the derivative at x={x_val} is: {derivative_at_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Going Multivariate: The Gradient (âˆ‡)\n",
    "\n",
    "Most functions in machine learning have many inputs. The **gradient** is a vector that contains the partial derivative with respect to each input. It points in the direction of the steepest ascent of the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Calculating a Gradient\n",
    "\n",
    "Consider the function $f(w_1, w_2) = 2w_1^2 + 3w_2^2$.\n",
    "\n",
    "Your task is to write a Python function `gradient_f` that takes a point `(w_1, w_2)` as a NumPy array and returns the gradient vector $\\nabla f = \\begin{bmatrix} \\frac{\\partial f}{\\partial w_1} \\\\ \\frac{\\partial f}{\\partial w_2} \\end{bmatrix}$ at that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_f(w):\n",
    "    \"\"\"\n",
    "    Calculates the gradient of f(w1, w2) = 2*w1^2 + 3*w2^2.\n",
    "    \n",
    "    Args:\n",
    "        w (np.ndarray): A 2D NumPy array [w1, w2].\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: The gradient vector [df/dw1, df/dw2].\n",
    "    \"\"\"\n",
    "    w1, w2 = w\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "# Calculate the gradient at the point (w1=1, w2=3)\n",
    "w_point = np.array([1, 3])\n",
    "grad_vector = gradient_f(w_point)\n",
    "\n",
    "print(f\"The gradient at {w_point} is: {grad_vector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. The Chain Rule: Backpropagation's Secret Sauce \n",
    "\n",
    "Neural networks are nested functions. To find the derivative of the loss with respect to a weight deep inside, we use the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1: A Manual Backpropagation Step\n",
    "\n",
    "Imagine a tiny piece of a neural network:\n",
    "1.  A neuron computes a value: $a = wx + b$\n",
    "2.  An activation function is applied: $h = \\text{sigmoid}(a)$\n",
    "3.  A squared error loss is computed: $L = (y - h)^2$\n",
    "\n",
    "Using the chain rule, we know that $\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial h} \\cdot \\frac{\\partial h}{\\partial a} \\cdot \\frac{\\partial a}{\\partial w}$.\n",
    "\n",
    "Your task is to calculate each of these derivatives and combine them to find the final gradient `dL_dw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Let's assume some values for our variables\n",
    "w = 0.5\n",
    "x = 2.0\n",
    "b = -1.0\n",
    "y = 1.0 # The true label\n",
    "\n",
    "# --- FORWARD PASS ---\n",
    "a = w * x + b\n",
    "h = sigmoid(a)\n",
    "L = (y - h)**2\n",
    "\n",
    "print(f\"Forward Pass Results:\")\n",
    "print(f\"a = {a:.4f}, h = {h:.4f}, L = {L:.4f}\")\n",
    "\n",
    "# --- BACKWARD PASS (GRADIENT CALCULATION) ---\n",
    "\n",
    "# 1. Calculate dL/dh (Derivative of loss w.r.t. h)\n",
    "dL_dh = # YOUR CODE HERE\n",
    "\n",
    "# 2. Calculate dh/da (Derivative of sigmoid w.r.t. a)\n",
    "dh_da = # YOUR CODE HERE\n",
    "\n",
    "# 3. Calculate da/dw (Derivative of 'a' w.r.t. w)\n",
    "da_dw = # YOUR CODE HERE\n",
    "\n",
    "# 4. Combine them using the chain rule\n",
    "dL_dw = # YOUR CODE HERE\n",
    "\n",
    "print(f\"\\nBackward Pass Gradient:\")\n",
    "print(f\"dL/dw = {dL_dw:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Probability: Handling Uncertainty\n",
    "\n",
    "The next sections will cover foundational probability concepts that are crucial for understanding and building machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.1: The Frequentist Approach in Action\n",
    "\n",
    "The frequentist view defines probability as the long-run frequency of an event. Let's simulate this by flipping a virtual coin.\n",
    "\n",
    "Your task is to simulate flipping a fair coin `n` times and calculate the estimated probability of getting heads. Use `np.random.rand()` which gives a random number in `[0, 1)`. Assume heads if the number is `< 0.5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_heads_probability(n):\n",
    "    \"\"\"\n",
    "    Simulates n coin flips and returns the estimated probability of heads.\n",
    "    \"\"\"\n",
    "    # Generate n random numbers between 0 and 1\n",
    "    flips = np.random.rand(n)\n",
    "    \n",
    "    # Count how many of these are 'heads' (e.g., < 0.5)\n",
    "    num_heads = # YOUR CODE HERE\n",
    "    \n",
    "    # Calculate the estimated probability\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "# Test the function with different numbers of flips\n",
    "print(f\"Estimated P(Heads) with 10 flips: {estimate_heads_probability(10)}\")\n",
    "print(f\"Estimated P(Heads) with 1000 flips: {estimate_heads_probability(1000)}\")\n",
    "print(f\"Estimated P(Heads) with 1000000 flips: {estimate_heads_probability(1000000)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.2: The Bayesian Approach with Code\n",
    "\n",
    "Bayesian inference is about updating our beliefs. Let's solve a classic problem:\n",
    "\n",
    "**Problem:** A medical test for a disease is 99% accurate (it's positive for 99% of people with the disease and negative for 99% of people without it). The disease affects 1% of the population. If a patient tests positive, what is the actual probability they have the disease?\n",
    "\n",
    "Use Bayes' Theorem: $P(\\text{Disease} | \\text{Positive}) = \\frac{P(\\text{Positive} | \\text{Disease}) \\cdot P(\\text{Disease})}{P(\\text{Positive})}$\n",
    "\n",
    "Where $P(\\text{Positive}) = P(\\text{Positive} | \\text{Disease})P(\\text{Disease}) + P(\\text{Positive} | \\text{No Disease})P(\\text{No Disease})$.\n",
    "\n",
    "Your task is to fill in the variables and calculate the posterior probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given information\n",
    "p_disease = 0.01\n",
    "p_pos_given_disease = 0.99\n",
    "p_pos_given_no_disease = 0.01\n",
    "\n",
    "# 1. Calculate P(No Disease)\n",
    "p_no_disease = # YOUR CODE HERE\n",
    "\n",
    "# 2. Calculate the total probability of testing positive, P(Positive)\n",
    "p_positive = # YOUR CODE HERE\n",
    "\n",
    "# 3. Apply Bayes' Theorem to find the posterior probability\n",
    "p_disease_given_pos = # YOUR CODE HERE\n",
    "\n",
    "print(f\"Prior probability of having the disease: {p_disease:.2%}\")\n",
    "print(f\"After testing positive, the updated probability is: {p_disease_given_pos:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Jacobians and Probability Distributions\n",
    "\n",
    "Let's move on to some more challenging problems that are highly relevant in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.1: The Jacobian Matrix\n",
    "\n",
    "The Jacobian matrix is the generalization of the gradient for vector-valued functions. It represents the best linear approximation of the function at a point.\n",
    "\n",
    "Consider the function $f(x, y) = \\begin{bmatrix} f_1(x, y) \\\\ f_2(x, y) \\end{bmatrix} = \\begin{bmatrix} x^2 \\sin(y) \\\\ y^2 \\cos(x) \\end{bmatrix}$.\n",
    "\n",
    "The Jacobian matrix $J$ is defined as $J = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x} & \\frac{\\partial f_1}{\\partial y} \\\\ \\frac{\\partial f_2}{\\partial x} & \\frac{\\partial f_2}{\\partial y} \\end{bmatrix}$.\n",
    "\n",
    "**Your Task:** Write a function that computes the Jacobian matrix for $f(x, y)$ at a given point `(x, y)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_jacobian(point):\n",
    "    \"\"\"\n",
    "    Computes the Jacobian of f(x,y) = [x^2*sin(y), y^2*cos(x)].\n",
    "    Args:\n",
    "        point (np.ndarray): A 2D NumPy array [x, y].\n",
    "    Returns:\n",
    "        np.ndarray: The 2x2 Jacobian matrix.\n",
    "    \"\"\"\n",
    "    x, y = point\n",
    "    \n",
    "    # Partial derivatives of f1 = x^2 * sin(y)\n",
    "    df1_dx = # YOUR CODE HERE\n",
    "    df1_dy = # YOUR CODE HERE\n",
    "    \n",
    "    # Partial derivatives of f2 = y^2 * cos(x)\n",
    "    df2_dx = # YOUR CODE HERE\n",
    "    df2_dy = # YOUR CODE HERE\n",
    "    \n",
    "    # Assemble the Jacobian matrix\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "# Compute the Jacobian at (x=pi/2, y=pi/3)\n",
    "point = np.array([np.pi/2, np.pi/3])\n",
    "jacobian = compute_jacobian(point)\n",
    "\n",
    "print(f\"Point (x, y): ({point[0]:.2f}, {point[1]:.2f})\")\n",
    "print(f\"Jacobian matrix:\\n{jacobian}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.2: Joint and Conditional Probability\n",
    "\n",
    "Let's analyze a dataset of weather and activity choices. The **joint probability** $P(\\text{Weather}, \\text{Activity})$ is given in the table below.\n",
    "\n",
    "|            | Walk  | Shop  | Clean |\n",
    "|------------|-------|-------|-------|\n",
    "| **Sunny** | 0.25  | 0.15  | 0.05  |\n",
    "| **Cloudy** | 0.10  | 0.10  | 0.10  |\n",
    "| **Rainy** | 0.00  | 0.05  | 0.20  |\n",
    "\n",
    "**Your Tasks:**\n",
    "1.  Calculate the **marginal probability** of it being 'Rainy', $P(\\text{Weather=Rainy})$.\n",
    "2.  Calculate the **conditional probability** of choosing to 'Clean' given that it is 'Rainy', $P(\\text{Activity=Clean} | \\text{Weather=Rainy})$.\n",
    "\n",
    "*Hint: $P(A|B) = \\frac{P(A, B)}{P(B)}$*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joint probability table P(Weather, Activity)\n",
    "# Rows: 0:Sunny, 1:Cloudy, 2:Rainy\n",
    "# Cols: 0:Walk,  1:Shop,   2:Clean\n",
    "joint_prob = np.array([\n",
    "    [0.25, 0.15, 0.05], # Sunny\n",
    "    [0.10, 0.10, 0.10], # Cloudy\n",
    "    [0.00, 0.05, 0.20]  # Rainy\n",
    "])\n",
    "\n",
    "# 1. Calculate the marginal probability P(Weather=Rainy)\n",
    "# Hint: Sum the probabilities in the 'Rainy' row.\n",
    "p_rainy = # YOUR CODE HERE\n",
    "print(f\"Marginal probability P(Weather=Rainy): {p_rainy:.2f}\")\n",
    "\n",
    "# 2. Calculate the conditional probability P(Activity=Clean | Weather=Rainy)\n",
    "# Hint: Use the formula P(A|B) = P(A and B) / P(B)\n",
    "# P(Activity=Clean and Weather=Rainy) is a value from the table.\n",
    "p_clean_and_rainy = # YOUR CODE HERE\n",
    "p_clean_given_rainy = # YOUR CODE HERE\n",
    "print(f\"Conditional probability P(Activity=Clean | Weather=Rainy): {p_clean_given_rainy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.3: Expected Value and Variance\n",
    "\n",
    "The **expected value** $E[X]$ is the long-term average value of a random variable, while the **variance** $\\text{Var}(X)$ measures its spread or dispersion.\n",
    "\n",
    "* $E[X] = \\sum_i x_i P(x_i)$\n",
    "* $\\text{Var}(X) = E[(X - E[X])^2] = \\sum_i (x_i - E[X])^2 P(x_i)$\n",
    "\n",
    "Consider a loaded die where the outcomes and their probabilities are:\n",
    "* Outcomes `X`: `[1, 2, 3, 4, 5, 6]`\n",
    "* Probabilities `P(X)`: `[0.1, 0.1, 0.1, 0.1, 0.1, 0.5]`\n",
    "\n",
    "**Your Task:** Write functions to calculate the expected value and variance for this loaded die."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes = np.array([1, 2, 3, 4, 5, 6])\n",
    "probs = np.array([0.1, 0.1, 0.1, 0.1, 0.1, 0.5])\n",
    "\n",
    "def calculate_expected_value(x, p):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def calculate_variance(x, p):\n",
    "    # Hint: You'll need the expected value first.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "expected_value = calculate_expected_value(outcomes, probs)\n",
    "variance = calculate_variance(outcomes, probs)\n",
    "\n",
    "print(f\"The expected value of the loaded die is: {expected_value:.2f}\")\n",
    "print(f\"The variance of the loaded die is: {variance:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
